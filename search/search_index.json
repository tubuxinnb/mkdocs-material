{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Blog","text":""},{"location":"2025/12/07/how-to-write-statement-of-purpose/","title":"How To Write Statement of Purpose","text":"<p>Abstract</p> <p>\u5199\u4e00\u7bc7\u4f18\u79c0\u7684 Statement of Purpose (SoP) \u662f\u7559\u5b66\u7533\u8bf7\uff08\u5c24\u5176\u662f\u7855\u58eb\u548c\u535a\u58eb\u7533\u8bf7\uff09\u4e2d\u6700\u5173\u952e\u7684\u4e00\u73af\u3002\u5b83\u4e0d\u662f\u4f60\u7b80\u5386\uff08CV\uff09\u7684\u6269\u5145\u7248\uff0c\u800c\u662f\u4e00\u7bc7\u903b\u8f91\u4e25\u5bc6\u7684\u8bae\u8bba\u6587\u3002</p> <p>\u4f60\u9700\u8981\u5411\u62db\u751f\u59d4\u5458\u4f1a\uff08AdCom\uff09\u8bc1\u660e\u4e09\u4ef6\u4e8b\uff1a 1. \u4f60\u4e3a\u4ec0\u4e48\u60f3\u5b66\u8fd9\u4e2a\uff1f 2. \u4f60\u4e3a\u4ec0\u4e48\u80fd\u5b66\u597d\uff1f 3. \u4f60\u4e3a\u4ec0\u4e48\u9009\u6211\u4eec\u5b66\u6821\uff1f</p> <p>\u4ee5\u4e0b\u662f\u64b0\u5199\u9ad8\u8d28\u91cf SoP \u7684\u7cfb\u7edf\u6307\u5357\u548c\u7ed3\u6784\u5efa\u8bae\uff1a</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#mindset","title":"\u7b2c\u4e00\u9636\u6bb5\uff1a\u6838\u5fc3\u601d\u7ef4 (Mindset)","text":"<ol> <li>\u5b83\u4e0d\u4ec5\u4ec5\u662f\u201c\u76ee\u7684\u9648\u8ff0\u201d\uff1a \u5b83\u5176\u5b9e\u662f Statement of Preparation (\u51c6\u5907\u9648\u8ff0) \u548c Statement of Potential (\u6f5c\u529b\u9648\u8ff0)\u3002</li> <li>\u62d2\u7edd\u6d41\u6c34\u8d26\uff1a \u4e0d\u8981\u6309\u65f6\u95f4\u987a\u5e8f\u7f57\u5217\u4f60\u4ece\u5c0f\u5230\u5927\u505a\u4e86\u4ec0\u4e48\u3002\u8981\u63d0\u70bc\u51fa\u4e00\u6761\u4e3b\u7ebf (Storyline)\uff0c\u5c06\u4f60\u7684\u7ecf\u5386\u4e32\u8054\u8d77\u6765\u3002</li> <li>Show, Don't Tell\uff1a \u4e0d\u8981\u8bf4 \"I am passionate about...\"\uff0c\u8981\u63cf\u8ff0\u4e00\u4e2a\u4f60\u4e3a\u4e86\u89e3\u51b3\u67d0\u4e2a\u96be\u9898\u5e9f\u5bdd\u5fd8\u98df\u7684\u5177\u4f53\u573a\u666f\u3002</li> </ol>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#structure","title":"\u7b2c\u4e8c\u9636\u6bb5\uff1a\u884c\u6587\u7ed3\u6784 (Structure)","text":"<p>\u4e00\u7bc7\u6807\u51c6\u7684 SoP \u901a\u5e38\u5728 800-1000 \u5b57\u5de6\u53f3\uff08\u7ea6 1.5 - 2 \u9875\uff09\uff0c\u5efa\u8bae\u91c7\u7528 \u5012\u91d1\u5b57\u5854\u7ed3\u6784\uff1a</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#1-the-hook-10-15","title":"1. \u5f00\u7bc7 (The Hook) - \u7ea6 10-15%","text":"<p>\u76ee\u6807\uff1a \u6293\u4f4f\u773c\u7403\uff0c\u5f15\u51fa\u7814\u7a76\u5174\u8da3\u3002 * \u4e0d\u8981\u5199\uff1a \"I have always been interested in computer science since I was a child.\" (\u592a\u8001\u5957) * \u8981\u5199\uff1a \u4ece\u4e00\u4e2a\u5177\u4f53\u7684\u73b0\u8c61\u3001\u4e00\u4e2a\u56f0\u6270\u4f60\u7684\u5b66\u672f\u95ee\u9898\u3001\u6216\u8005\u4e00\u6bb5\u72ec\u7279\u7684\u7ecf\u5386\u5207\u5165\u3002 * \u793a\u4f8b\uff1a \"\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u5e76\u53d1\u65f6\uff0c\u6211\u53d1\u73b0\u73b0\u6709\u7684\u5185\u5b58\u5206\u914d\u673a\u5236\u5b58\u5728\u660e\u663e\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u8fd9\u6fc0\u53d1\u4e86\u6211\u5bf9\u7cfb\u7edf\u67b6\u6784\u4f18\u5316\u7684\u6df1\u5165\u601d\u8003...\" * \u7ed3\u5c3e\uff1a \u7b80\u660e\u627c\u8981\u5730\u63d0\u51fa\u4f60\u7684\u77ed\u671f\u548c\u957f\u671f\u76ee\u6807\u3002</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#2-the-meat-40-50","title":"2. \u5b66\u672f/\u7814\u7a76\u80cc\u666f (The Meat) - \u7ea6 40-50%","text":"<p>\u76ee\u6807\uff1a \u8bc1\u660e\u4f60\u5177\u5907\u5b8c\u6210\u5b66\u4e1a/\u7814\u7a76\u7684\u80fd\u529b (Competence)\u3002 \u8fd9\u662f\u6700\u91cd\u8981\u7684\u90e8\u5206\u3002\u6311\u9009 2-3 \u4e2a\u6700\u76f8\u5173\u7684\u9879\u76ee\u6216\u7ecf\u5386\uff08\u4e0d\u8981\u9762\u9762\u4ff1\u5230\uff09\u3002 * \u5199\u4f5c\u516c\u5f0f\uff1a STAR \u6cd5\u5219 (Situation, Task, Action, Result) + R (Reflection)     * Situation: \u9047\u5230\u4e86\u4ec0\u4e48\u96be\u9898\uff1f     * Action: \u4f60\u5177\u4f53\u7528\u4e86\u4ec0\u4e48\u6280\u672f\u3001\u65b9\u6cd5\u89e3\u51b3\u4e86\u5b83\uff1f\uff08\u4f53\u73b0\u786c\u6838\u6280\u80fd\uff09     * Result: \u7ed3\u679c\u5982\u4f55\uff1f\uff08\u53d1\u8868\u4e86\u8bba\u6587\uff1f\u63d0\u5347\u4e86\u6548\u7387\uff1f\u83b7\u5f97\u4e86\u5956\u9879\uff1f\uff09     * Reflection (\u6700\u91cd\u8981): \u8fd9\u6bb5\u7ecf\u5386\u8ba9\u4f60\u5bf9\u8be5\u9886\u57df\u6709\u4e86\u4ec0\u4e48\u65b0\u7684\u8ba4\u77e5\uff1f\u5b83\u5982\u4f55\u4e3a\u4f60\u672a\u6765\u7684\u7814\u7a76\u751f\u5b66\u4e60\u6253\u4e0b\u57fa\u7840\uff1f</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#3-the-fit-20-25","title":"3. \u4e3a\u4ec0\u4e48\u9009\u62e9\u8be5\u9879\u76ee/\u5b66\u6821 (The Fit) - \u7ea6 20-25%","text":"<p>\u76ee\u6807\uff1a \u8bc1\u660e\u4f60\u4e0d\u662f\u6d77\u6295\uff0c\u4f60\u662f\u771f\u7684\u7814\u7a76\u8fc7\u4ed6\u4eec\u3002 * \u5b9a\u5236\u5316\uff1a \u8fd9\u662f\u533a\u5206 \"\u901a\u7528\u6a21\u7248\" \u548c \"\u7528\u5fc3\u7533\u8bf7\" \u7684\u5173\u952e\u3002 * \u63d0\u53ca\u6559\u6388\uff1a (\u7279\u522b\u662f PhD \u7533\u8bf7) \u660e\u786e\u6307\u51fa\u4f60\u60f3\u548c\u54ea\u4f4d\u6559\u6388\u5408\u4f5c\uff0c\u5e76\u89e3\u91ca\u539f\u56e0\u3002     * \u793a\u4f8b\uff1a \"Prof. X's work on [Topic] aligns perfectly with my interest in [Your Interest]. I am particularly intrigued by his recent paper on...\" * \u63d0\u53ca\u8bfe\u7a0b/\u8d44\u6e90\uff1a \u63d0\u5230\u8be5\u6821\u7279\u5b9a\u7684\u5b9e\u9a8c\u5ba4\u3001\u8bfe\u7a0b\u8bbe\u7f6e\u6216\u8de8\u5b66\u79d1\u673a\u4f1a\u3002</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#4-future-goals-10-15","title":"4. \u804c\u4e1a\u76ee\u6807\u4e0e\u7ed3\u8bed (Future Goals) - \u7ea6 10-15%","text":"<p>\u76ee\u6807\uff1a \u5c55\u793a\u4f60\u7684\u8fdc\u89c1\u3002 * \u8bf4\u660e\u8be5\u5b66\u4f4d\u5982\u4f55\u5e2e\u52a9\u4f60\u5b9e\u73b0\u804c\u4e1a\u76ee\u6807\uff08\u8fdb\u5165\u5b66\u672f\u754c\u4efb\u6559\uff1f\u8fd8\u662f\u8fdb\u5165\u5de5\u4e1a\u754c\u7814\u53d1\u90e8\u95e8\uff1f\uff09\u3002 * \u7528\u4e00\u4e2a\u6709\u529b\u3001\u81ea\u4fe1\u7684\u53e5\u5b50\u603b\u7ed3\u5168\u6587\u3002</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#dos-and-donts","title":"\u7b2c\u4e09\u9636\u6bb5\uff1a\u5199\u4f5c\u4e2d\u7684 \"Do's and Don'ts\"","text":"\u2705 \u8981\u505a (Do's) \u274c \u522b\u505a (Don'ts) \u5177\u4f53\u5316\uff1a \u4f7f\u7528\u6570\u636e\u3001\u5177\u4f53\u7684\u7f16\u7a0b\u8bed\u8a00\u3001\u5177\u4f53\u7684\u7406\u8bba\u6a21\u578b\u3002 \u60c5\u7eea\u5316/\u717d\u60c5\uff1a \u907f\u514d\u8fc7\u5ea6\u63cf\u5199\u7ae5\u5e74\u68a6\u60f3\u6216\u5bb6\u5ead\u60b2\u5267\uff0c\u9664\u975e\u4e0e\u4e13\u4e1a\u5f3a\u76f8\u5173\u3002 \u8fde\u63a5\u70b9 (Connections)\uff1a \u4f7f\u7528\u8fc7\u6e21\u53e5\uff0c\u8ba9\u6bb5\u843d\u4e4b\u95f4\u6d41\u7545\u8854\u63a5\uff0c\u800c\u4e0d\u662f\u5272\u88c2\u7684\u677f\u5757\u3002 \u91cd\u590d\u7b80\u5386\uff1a \u4e0d\u8981\u628a CV \u91cc\u7684 bullet points \u6269\u5199\u6210\u53e5\u5b50\u3002SoP \u8bb2\u7684\u662f Why \u548c How\uff0cCV \u8bb2\u7684\u662f What\u3002 \u6b63\u89c6\u5f31\u70b9\uff1a \u5982\u679c GPA \u6709\u786c\u4f24\uff0c\u53ef\u4ee5\u7b80\u77ed\u5ba2\u89c2\u5730\u89e3\u91ca\uff08\u5982\u751f\u75c5\u3001\u5bb6\u5ead\u53d8\u6545\uff09\uff0c\u5e76\u5f3a\u8c03\u540e\u6765\u7684\u63d0\u5347\u3002 \u4f7f\u7528\u590d\u6742\u957f\u96be\u53e5\uff1a \u4e0d\u8981\u4e3a\u4e86\u663e\u5f97\u82f1\u8bed\u597d\u800c\u5806\u780c\u4ece\u53e5\u3002\u6e05\u6670 (Clarity) \u6c38\u8fdc\u662f\u7b2c\u4e00\u4f4d\u7684\u3002 \u9488\u5bf9\u6027\uff1a \u4e3a\u6bcf\u4e00\u6240\u5b66\u6821\u4fee\u6539 \"Why School\" \u90e8\u5206\u3002 \u6284\u88ad/\u5957\u6a21\u7248\uff1a \u62db\u751f\u5b98\u770b\u8fc7\u6210\u5343\u4e0a\u4e07\u4efd\u6587\u4e66\uff0c\u6a21\u7248\u75d5\u8ff9\u4e00\u773c\u5c31\u80fd\u770b\u7a7f\u3002"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#polishing","title":"\u7b2c\u56db\u9636\u6bb5\uff1a\u6da6\u8272\u4e0e\u68c0\u67e5 (Polishing)","text":"<ol> <li>\u903b\u8f91\u68c0\u67e5\uff1a \u6bcf\u4e00\u6bb5\u662f\u5426\u90fd\u670d\u52a1\u4e8e\u4f60\u7684\u6838\u5fc3\u8bba\u70b9\uff08\u6211\u6709\u80fd\u529b\u4e14\u9002\u5408\u8fd9\u4e2a\u9879\u76ee\uff09\uff1f</li> <li>\u5ba2\u89c2\u68c0\u67e5\uff1a \u6240\u6709\u7684\u5f62\u5bb9\u8bcd\uff08passionate, dedicated, excellent\uff09\u662f\u5426\u6709\u4e8b\u5b9e\u652f\u6491\uff1f\u5982\u679c\u5220\u6389\u8fd9\u4e9b\u5f62\u5bb9\u8bcd\uff0c\u4e8b\u5b9e\u672c\u8eab\u662f\u5426\u4f9d\u7136\u6709\u529b\uff1f</li> <li>\u8bed\u8a00\u68c0\u67e5\uff1a<ul> <li>\u627e\u6bcd\u8bed\u8005\u6216\u4e13\u4e1a\u673a\u6784\u6da6\u8272\u8bed\u6cd5\u3002</li> <li>\u4f7f\u7528\u5b66\u672f\u8bcd\u6c47\uff08\u5982\uff1aInstead of \"I thought about,\" use \"I analyzed/hypothesized\"\uff09\u3002</li> </ul> </li> </ol>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#roadmap","title":"Roadmap","text":""},{"location":"2025/12/07/how-to-write-statement-of-purpose/#hook","title":"Hook","text":"<ol> <li>Unique Experience: Remote collaboration-Zurich &amp; China</li> <li>Confusing academic problems: simulation &amp; searching for ideas</li> <li>Arise the research interets: Advanced Computer Architecture for Emerging Applications such as AI &amp; ML.</li> </ol>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#academic-background","title":"Academic Background","text":"<ol> <li>Just focus on the new architecture--DATE</li> <li>\u7cfb\u7edf&amp;\u786c\u4ef6\u67b6\u6784\u534f\u540c\u8bbe\u8ba1--DAC</li> <li>focus on emerging system &amp; compatible architecture--Next to NPU (GPU &amp; TPU) &amp; emerging ML systems</li> </ol>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#fit","title":"Fit","text":"<p>\u540e\u9762\u518d\u5199\uff0c\u6839\u636e\u5b66\u6821\u548c\u8001\u5e08\u6765</p>"},{"location":"2025/12/07/how-to-write-statement-of-purpose/#future-goals","title":"Future Goals","text":"<p>\u5b66\u672f\u754c\u4efb\u6559\uff0c\u6307\u5bfc\u66f4\u591a\u7684\u5b66\u751f\uff0c\u6307\u8def\u4eba</p>"},{"location":"2025/12/05/summary-of-inquiries/","title":"Summary of Inquiries","text":"<p>\u660e\u5929\u6700\u540e\u4e00\u6b21\u6258\u798f\u8003\u8bd5\uff0c\u603b\u7ed3\u4e00\u4e0b\u5df2\u7ecf\u5957\u78c1\u7684\u8001\u5e08\u548c\u53cd\u9988\u60c5\u51b5</p> <p>UCSD\uff1aYufei Ding\uff0c\u6709\u56de\u590d\uff0c\u6709\u9762\u8bd5\uff0c\u88ab\u723d\u55b7\u4f46\u804a\u7684\u4e5f\u633a\u6709\u6536\u83b7\uff0c\u4f30\u8ba1\u6ca1\u5e0c\u671b\u4f46\u53ef\u4ee5\u7533\u8bf7\u8bd5\u8bd5</p> <p>UCR\uff1aBingyao Li\uff0c\u6709\u56de\u590d\uff0c\u6709meeting\uff0c\u5e0c\u671b\u8fd8\u633a\u5927</p> <p>PITT\uff1aXulong Tang\uff0c\u4e00\u6708\u804a\uff0c\u5e0c\u671b\u6709\u5e0c\u671b</p> <p>CUHK\uff1aZhengrong Wang\uff0c\u6709\u56de\u590d\uff0c\u6709meeting\uff0c\u611f\u89c9\u662f\u4f5b\u7cfb\u62db\u751f\uff0c\u5e94\u8be5\u5728\u6162\u6162\u6392\u5e8f</p> <p>HKUST\uff1achenzizhong\uff0c\u4e0d\u8fc7\u597d\u50cf\u8fd8\u5728UCR</p> <p>CUHK (SZ)\uff1ayuminchen\uff0c\u4e5f\u9e3d\u4e86</p> <p>HKUST (GZ)\uff1a\u5f20\u575a\uff0c\u597d\u56de\u590d\u4f46\u9e3d\u4e86</p> <p>SJTU: \u51af\u5b87\uff0c\u804a\u7684\u8fd8\u884c\uff0c\u4e24\u5468\u540e\u7ea6</p> <p>VUA: \u65e0\u56de\u590d</p> <p>\u8003\u5b8c\u6258\u798f\u53ef\u4ee5\u7ee7\u7eed\u5957\u78c1\u7684\u8001\u5e08\uff1a</p> <p>PSU\uff1asystem lab\u6311\u51e0\u4e2a\u987a\u773c\u7684\u5957</p> <p>UCI\uff1asitao huang</p> <p>UCSC: yuanchao Xu</p> <p>Rice: Yuke Wang</p> <p>USF: Jiajun huang</p> <p>NYU: Saiqian Zhang</p>"},{"location":"2025/12/01/speaking-task-2/","title":"Speaking Task 2","text":"<p>Answer Instructions </p>"},{"location":"2025/12/01/speaking-task-2/#reading-material","title":"Reading Material","text":"<ol> <li>Definition (Priority 1)</li> <li>Funcion (Priority 2)</li> <li>Orders of Events (If time allows)</li> </ol> <p>For Example: </p> <p>Relict Behaviors</p> <p>Typically, animals behave in certain ways to ensure that they survive in their natural habitats. But there are instances in which an animal species might exhibit a behavior that doesn't seem to have an obvious use anymore (function here, other supplement is unnecessary), in fact, it probably hasn't fulfilled its original purpose for a very long time, maybe even thousands of years. Such behaviors, called relict behaviors, were beneficial to the animals when the habitat they lived in was different; however, now, because the habitat has changed, that behavior no longer fulfills its original function. Long after the environmental situation that shaped its evolution has disappeared, the behavior remains, like a relic, a remnant, left from long, long ago.</p> <p>Def: the behavior which has no obvious use.</p> <p>Fun: It is shaped long time ago, now the habitat has changed, the behavior no longer fulfills its original funcitons.</p>"},{"location":"2025/12/01/speaking-task-2/#listening-lecture","title":"Listening Lecture","text":"<p>Content: An example of the concept</p> <p>Key: Listen for hyponyms of the reading info</p> <p>Answer Structure: </p> <ol> <li>Summarize the reading material:</li> </ol> <p>\u200b   xxx is xxxx</p> <ol> <li> <p>The lecture' s Example: </p> </li> <li> <p>The professor take xxx to illustrate/ for example.</p> </li> <li> <p>More details and concrete words are better!!!</p> </li> <li> <p>Usually, the example's structue is likely that before the xxx (indicated by the concept), xxx, after that, xxx. </p> </li> <li>At the end, do a summary like: so this xxx is a xxx (concept).</li> </ol>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/","title":"Akita--A Go-based Simulation Engine","text":"<p>Akita\u4f5c\u4e3aMGPU-SIM\u7684\u57fa\u5ea7\uff0c\u4e3a\u7814\u7a76Multi-GPU Arch\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u6a21\u62df&amp;\u8bc4\u4f30\u5e73\u53f0\uff0c\u540e\u7eed\u5c1d\u8bd5\u57fa\u4e8e\u6b64\u505a\u4e00\u4e9bGPU Arch Research\u3002</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#event-driven-simulation","title":"Event-Driven Simulation","text":""},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#time-record","title":"Time Record","text":"<p>In most simulators, time is typically recorded using u64 bit int.</p> <p>However, in Akita, considering: 1. Dynamic Voltage and Frequency Scaling (DVFS) 2. multiple frequency domains 3. memory system &amp; cores run at different frequencies</p> <pre><code>type VTimeInSec float64\n</code></pre> <p>V: Virtual Time, not real time. InSec: the time unit in Akita is consistently in seconds.</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#frequency-clock-cycles","title":"Frequency &amp; clock cycles","text":"<p>Quote</p> <p>Digital circuit update their states at clock cycle boundaries (ticks). These clock cycles usually maintain a steady frequency, which defines how soon digital circuit can update their status. This requires the simulator keep needing to calculate the time of the next cycle boundary. To address the need, Akita provides a <code>frequency</code> type.</p> <p>Akita provides some relative functions including <code>period</code>, <code>ThisTick</code> and <code>NextTick</code>.</p> <pre><code>type Freq float64\n\nfunc (f Freq) ThisTick(now VTimeInSec) VTimeInSec\nfunc (f Freq) NextTick(now VTimeInSec) VTimeInSec\n</code></pre> <p></p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#events","title":"Events","text":"<p>In Akita, an event is an action without lasting time. So the action happens instantaneously.</p> <p>A event has 2 mandatory properites: the time and handler, which are both immutable, once assigned and notupdated afterward.</p> <pre><code>type Event interface {\n    // Return the time that the event should happen\n    Time() VTimeInSec\n\n    // Returns the handler that should handle the event\n    Handler() Handler\n\n    // IsSecondary tells if the event is a secondary event. Secondary event are\n    // handled after all same-time primary events are handled.\n    IsSecondary() bool\n}\n</code></pre> <p>where <code>Handler</code> is always developed and difined by simulator developers.</p> <pre><code>type Handler interface {\n    Handle(e Event) error\n}\n</code></pre>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#event-driven-simulation-engine","title":"Event-Driven Simulation Engine","text":"<p>!!! quote Event-Driven -&gt; Engine</p> <pre><code>Since we have defined event, a simulation can be considered as replaying a list of events in chronological order. Such a player is called an event-driven simulation engine, or engine for short. The engine maintains a queue of events and triggers events one by one.\n</code></pre> <p>2 functions in engine interfaces: <code>Run</code> and <code>Schedule</code>.</p> <ul> <li><code>Run</code>: Run triggers all the events (handled by handlers) until no event is left in the event queue.</li> <li><code>Schedule</code>:  registers an event to be triggered in the future.</li> </ul> <p>2 implementations of engines: serial &amp; parallel engines.</p> <p>!!! quote Why not cycle drvien simulation?</p> <pre><code>First, cycle driven simulation is typically slower than event driven mainly because event-driven simulation can skip most **useless cycles**.\nSecond, cycle-based simulation has challenges handling systems with different frequency domains.\n</code></pre>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#a-simple-example","title":"A simple Example","text":"<p>Suppose we have one small cell at the beginning. It will split at a random time between 1 - 2 second. After that, each cell will also wait for a random time between 1 - 2 seconds before the next split. We want to count the number of bacteria at the 10th second.</p> <p>between 1 - 2 second, how to present the random latency? the event: the waiting reaches the 1 or 2 second the handler: cell split every cell has own time record and global record to know when to terminate (10 second)</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#component","title":"Component","text":"<p>In system simulation, a large system typically is divided to several smaller related independent elements ,which are called component in Akita, such as computing cores, caches, and memory controllers.</p> <p>A full simulation requires the interaction between components, which is solved by a message-passing system in Akita.</p> <p>The transfer of messages relies on the ports and connections.</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#definition","title":"Definition","text":"<pre><code>type Component interface {\n    Named\n    Handler\n    Hookable\n    PortOwner\n\n    NotifyRecv(now VTimeInSec, port Port)\n    NotifyPortFree(now VTimeInSec, port Port)\n}\n</code></pre> <p><code>Named</code> defines a simple <code>Name() string</code> method.</p> <p>Rules: 1. Every component must have a name. 2. follows a hierarchy style: <code>GPU[3].SA[5].L1VCche[0]</code>. 3. capitalized camel case (\u9996\u5b57\u6bcd\u5927\u5199\uff0c\u9a7c\u5cf0\u5f0f) 4. square bracket indices, some rar cases have multi-dimensional indices.</p> <p>Actually, a component is a handler that can handle events.   Components typically schedule future events to be handled on their own. </p> <p>Example: Memory Controller schedule a R/W Request, then handle it in the future.</p> <p>### Message</p> <p>Quote</p> <p>Messages are the element that carries the information across components.</p> <pre><code>// A Msg is a piece of information transferred between components.\ntype Msg interface {\n    Meta() *MsgMeta\n}\n\n// MsgMeta contains the metadata that is attached to every message.\ntype MsgMeta struct {\n    ID                 string\n    Src, Dst           Port\n    SendTime, RecvTime VTimeInSec\n    TrafficClass       int\n    TrafficBytes       int\n}\n</code></pre> <p>Concrete message types can be self-definition like:</p> <pre><code>type ReadReq struct {\n    sim.MsgMeta\n\n    Address            uint64\n    AccessByteSize     uint64\n    PID                vm.PID\n    CanWaitForCoalesce bool\n    Info               interface{}\n}\n\n// Meta returns the message's metadata.\nfunc (r *ReadReq) Meta() *sim.MsgMeta {\n    return &amp;r.MsgMeta\n} \n</code></pre> <p>As developers defining new messages, they are actually also defining a protocol between the components.</p> <p>Quote</p> <p>We define the memory read protocol as a two step series: 1. the requester send the read request, and 2. the memory data provider response with a DataReadyResponse. The protocol also defines (documentation only) that no negative acknowledgement (NAK) or retry is allowed. The data provider should always respond with the data, regardless the latency.</p> <p>Considering responses as a special type. A general response is like:</p> <pre><code>type GeneralRsp struct {\n    MsgMeta\n\n    OriginalReq Msg //within a original Request msg\n}\n</code></pre> <p>Advantages: 1. simpified development 2. improve the visualization: easy to see where the origin is from.</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#ports","title":"Ports","text":"<pre><code>type Port interface {\n    ... // Other methods \n\n  // To send a message through the port.\n    CanSend() bool\n  Send(msg Msg) *SendError\n\n  // To receive a message from the port.\n  Retrieve(now VTimeInSec) Msg //receive and remove message here\n  Peek() Msg // receive but keep message here\n}\n</code></pre> <p>Ports can use <code>NotifyRecv(now VTimeInSec, port Port)</code> and <code>NotifyPortFree(now VTimeInSec, port Port)</code> to notice their components.</p> <p>Components always have multiple ports for various destinations. However, ports have only 1 component.</p>"},{"location":"2025/12/23/akita--a-go-based-simulation-engine/#connections","title":"Connections","text":"<p>Connections deliver messages from the source port to the destination port. </p> <p>A connection can connect multiple ports, but a port can only link with one connection.</p> <p>A connection can be considered a network (off-chip/on-chip) network that routes data across components.</p> <p>The connections will call the <code>SetConnection</code> method of the port to notify the port about the connection.</p>"},{"location":"2025/12/05/toefl-test-instruction/","title":"TOEFL TEST Instruction","text":"<p>12.5\uff0c\u6700\u540e\u8fc7\u4e00\u904d\u6240\u6709\u4e3a\u8003\u8bd5\u5df2\u7ecf\u51c6\u5907\u7684\u5185\u5bb9\uff0c\u6309\u8003\u8bd5\u79d1\u76ee\u987a\u5e8f-&gt;\u9898\u76ee\u8003\u67e5\u65b9\u5f0f\u56de\u5fc6\u5e76\u603b\u7ed3\u4e00\u4e0b</p>"},{"location":"2025/12/05/toefl-test-instruction/#reading-task","title":"Reading Task","text":"<ol> <li>\u4e00\u8fb9\u770b\u6587\u7ae0\u4e00\u8fb9\u505a\u9898</li> <li>\u8bcd\u6c47\u9898\u77e5\u9053\u539f\u610f\u5219\u6309\u539f\u610f\u9009\uff0c\u5426\u5219\u6309\u4e0a\u4e0b\u6587\u63a8\u65ad\u610f\u601d\u9009</li> <li>\u539f\u6587\u957f\u96be\u53e5\u603b\u7ed3\uff1a\u68b3\u7406\u51fa\u53e5\u5b50\u4e3b\u5e72\uff0c\u5220\u53bb\u591a\u4f59\u7684\u5b9a\u8bed\u72b6\u8bed\u4ece\u53e5\uff0c\u8868\u8fbe\u903b\u8f91\u770b\u6e05\u695a\uff0c\u53bb\u9009\u9879\u91cc\u627e\u540c\u6837\u7684\u903b\u8f91\u8868\u8fbe\u7684\u53e5\u5b50</li> <li>\u63a7\u5236\u65f6\u95f4\uff0c6\u90093\u53ef\u4ee5\u5feb\u901f\u9009\uff0c\u9519\u4e86\u6ca1\u5173\u7cfb</li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#listening-task","title":"Listening Task","text":"<ol> <li>\u6ce8\u610f\u65f6\u95f4\uff01\uff01\uff01\u4e00\u51712\u4e2asection\uff0c\u6bcf\u4e2asection 10min</li> <li>\u542c\u5230\u5565\u9009\u5565\uff0c\u6240\u6709\u9009\u9879\u6ca1\u542c\u5230\u5c31\u7a0d\u5fae\u82b1\u70b9\u65f6\u95f4\u731c\u4e00\u4e2a\uff08\u6709\u6982\u7387\u5176\u5b9e\u80fd\u731c\u5bf9\uff09</li> <li>\u4e0d\u8981\u7ea0\u7ed3\uff0c\u542c\u5230\u4e86\u5c31\u662f\u542c\u5230\u4e86\uff0c\u5f88\u5feb\u5c31\u80fd\u9009\u51fa\u6765\uff0c\u6ca1\u542c\u5230\u518d\u600e\u4e48\u7ea0\u7ed3\u4e5f\u6ca1\u6709\u7528hhh</li> <li>\u4e0d\u8981\u8fc7\u4e8e\u7d27\u5f20\uff0c\u80fd\u542c\u61c2\u591a\u5c11\u542c\u591a\u5c11\uff0c\u9002\u5f53\u8ddf\u8bfb\u4fdd\u6301\u6ce8\u610f\u529b\u96c6\u4e2d</li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#speaking-task","title":"Speaking Task","text":"<ol> <li> <p>\u5728\u8fdb\u5165\u754c\u9762\u70ed\u70ed\u8eab\u548c\u5634\u5df4\uff0c\u8bfb\u4e00\u8bfb</p> </li> <li> <p>\u51c6\u5907\u65f6\u95f4\u628a\u8349\u7a3f\u68b3\u7406\u597d\uff0c\u903b\u8f91\u68b3\u7406\u6e05\u695a\uff0c\u8868\u8fbe\u7684\u65f6\u5019\u4e00\u5b9a\u8981\u770b\u7740\u8349\u7a3f\u8bf4\uff0c\u4e0d\u8981\u51ed\u7a7a\u8bf4</p> </li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#task1","title":"Task1","text":"<ol> <li>State your point: Personally, I definitely agree/disagree that xxx. </li> <li>point 1:  xxx (Furthermore, and thus) It's helpful/essential for students/government/businesses/colleges/schools/parents </li> <li>point 2: An example. For example, without xxx, xxx. However, After xxx, xxx become xxx</li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#task2","title":"Task2","text":"<ol> <li>\u4fdd\u8bc1\u9605\u8bfb\u5b8c\u77ed\u6587\uff0c\u4e00\u822c\u6709\u4e00\u4e2aproposal/annoucement/notice &amp; 2 reason for it. \u6709\u65f6\u95f4\u7684\u8bdd\u5c06key words\u8bb0\u4e0b\u6765</li> <li>1.5min\u7684\u5bf9\u8bdd\uff0c\u6211\u4eec\u9700\u8981\u5173\u6ce8/\u5199\u4e0b\u6765</li> <li>\u6001\u5ea6\uff1aagree/disagree</li> <li>\u7406\u75311\uff1a\u4e3b\u5e72+\u7ec6\u8282/\u4f8b\u5b50\uff1a\u4e00\u822c\u6765\u8bf4\u90fd\u4f1a\u5bf9\u5e94\u5230\u9605\u8bfb\u6750\u6599\u91cc\u7684Reason</li> <li>\u7406\u75312:  \u4e3b\u5e72+\u7ec6\u8282/\u4f8b\u5b50\uff1a\u540c\u4e0a</li> </ol> <p>\u51c6\u5907\u9636\u6bb5\uff1a\u753b\u4e00\u5f20\u8868\uff0c\u5de6\u8fb9\u662fReading Material, \u53f3\u8fb9\u662fListening Material\uff0c\u7136\u540e\u5de6\u8fb9\u63d0\u524d\u5199\u597dRP, R1, R2\uff0c\u53f3\u8fb9\u5199\u597dAT, R1, E1, R2 E2.</p> <p>\u7b54\u9898\u9636\u6bb5\uff1a</p> <ol> <li>summarize the proposal: The proposal claims that xxx. Firstly because, xxxx. Secondly, xxxxx.</li> <li>mention the conversation attitude: However/And the man/woman think it's a / not good idea. </li> <li>Firstly, he mentioned that (Reason1). because Example1 follows.</li> <li>Moreover, he believed that (Reason2). because Example2 follows.</li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#task3","title":"Task3","text":"<ol> <li> <p>\u4fdd\u8bc1\u9605\u8bfb\u5b8c\u77ed\u6587\uff0c\u8bb0\u5f55Definition, Function &amp; event orders (Option)</p> </li> <li> <p>\u542c\u529b\u9636\u6bb5\uff0c\u4e00\u822c\u662f\u4e00\u4e2aProfessor\u4e3e\u4e00\u4e2a\u4f8b\u5b50\uff1a\u5728\u4e00\u4e2a\u573a\u666f\u4e0b\uff0c\u5979\u5148\u5e72\u4e86\u5565\uff0c\u540e\u8fd9\u4e2aDefinition\u7684function\u8d77\u4f5c\u7528\u4e86\uff0c\u5979\u53c8\u53bb\u5e72\u4e86\u5565\uff0c\u8bf4\u660e\u8fd9\u4e2a\u73b0\u8c61/term happens or exists.</p> </li> </ol> <p>\u51c6\u5907\u9636\u6bb5\uff1a\u5199\u6e05\u695adef\uff0cfunc\u548corder, \u7136\u540e\u628a\u542c\u529b\u9636\u6bb5\u7684\u4f8b\u5b50\u548corder of events\u5bf9\u4e0a\uff0c\u57fa\u672c\u4e0a\u662f\u539f\u6587\u7684\u4e0b\u4f4d\u8bcd/\u5b9e\u4f8b\u5316\uff08\u62bd\u8c61 &amp; \u5177\u8c61\uff09</p> <p>\u7b54\u9898\u9636\u6bb5\uff1a</p> <ol> <li>the reading talks about [TERM] and it states that [What's the TERM's FUNCTION]</li> <li>In the lecture, the professor mentions that .</li> <li>\u5728\u8fd9\u91cc\u6709\u4e24\u79cd\u60c5\u51b5\uff0c\u4e00\u4e2a\u4f8b\u5b50/\u5206\u6709\u524d\u540e\u903b\u8f91\uff0c\u5148\u600e\u4e48\u6837\uff0c\u7136\u540e\u600e\u4e48\u6837\uff1b\u4e24\u4e2a\u4f8b\u5b50\u5bf9\u6bd4\uff0c\u4e00\u4e2a\u6709/\u4e00\u4e2a\u6ca1\u6709</li> </ol>"},{"location":"2025/12/05/toefl-test-instruction/#task4","title":"Task4","text":"<ol> <li>\u542c\u4e00\u6bb5lecture\uff0c\u8bb2\u4e00\u4e2a\u5b66\u672f\u6982\u5ff5+2\u4e2a\u4f8b\u5b50</li> </ol> <p>\u7b54\u9898\u9636\u6bb5\uff1a</p> <ol> <li>In this lecture, the professor talks about xxx, which is xxxx</li> </ol> <p>Here are 2 examples for it.</p> <ol> <li>The first is that</li> <li>The second is that</li> </ol>"},{"location":"2025/12/07/3-matrix-hadamard-product/","title":"3 Matrix Hadamard Product","text":"<p>Hadamard Product is element-wise:  $$ A \\cdot B = C $$ where,  $$ C_{i,j} = A_{i,j} \\cdot B_{i,j} $$</p> <p>because every element in \\(A\\) and \\(B\\) is only calculated once. the Arithmetic Intensity (AI) is a constant. $$ FLOPS = N^2 $$ $$ Bytes = 2 \\times N^2 \\times 3~~(read~A, B, write~C) $$  $$ \\text{AI} = \\text{FLOPS} / \\text{Bytes} = \\frac{1}{6} $$ The Arithmetic Intensity is a fixed constant. So our goal is to improve the memory throughput of the dot product. We choose 2 ways to improve the memory throughput: 1. kernel fusion 2. memory coalescing for 3 matrix dot multiple, we should access 3 matrixes in memory. However, in Pytorch, this multiplication is implemented in 2 individual kernels of element-wise multiplication.</p>"},{"location":"2025/12/07/3-matrix-hadamard-product/#kernel-fusion","title":"Kernel Fusion","text":"<p>A intuitive way is to fuse these two kernels into one kernel:</p> <pre><code>__global__ void mul_tri_naive_kernel(const float* a, const float* b, const float* c, float* out, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        out[idx] = a[idx] * b[idx] * c[idx];\n    }\n}\n</code></pre> <p>where the elements in this kernel is not vectorized. </p>"},{"location":"2025/12/07/3-matrix-hadamard-product/#memory-coalescing","title":"Memory Coalescing","text":"<p>Considering that the memory access width of a single request is 128 bytes while the float's width is 32 bytes, there are 75% of the memory accesses are wasted. In my evaluation, as the size of matrix increases, the performance degrades severely.  However, we can use vectorized multiplication to improve the performance.  So we can coalesce 4 <code>float</code> into a single memory access using <code>float4</code> type:</p> <pre><code>__global__ void mul_tri_vec_kernel(const float* a, const float* b, const float* c, float* out, int n_vec) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n_vec) {\n        float4 va = reinterpret_cast&lt;const float4*&gt;(a)[idx];\n        float4 vb = reinterpret_cast&lt;const float4*&gt;(b)[idx];\n        float4 vc = reinterpret_cast&lt;const float4*&gt;(c)[idx];\n        float4 vout;\n        vout.x = va.x * vb.x * vc.x;\n        vout.y = va.y * vb.y * vc.y;\n        vout.z = va.z * vb.z * vc.z;\n        vout.w = va.w * vb.w * vc.w;\n        reinterpret_cast&lt;float4*&gt;(out)[idx] = vout;\n    }\n}\n</code></pre> <p>Comparison between different kernels: at Thread/Block Level:</p> Thread/Block kernel_num Block_num/Kernel Thread_num/Block Mul/Thread mem/Thread fuse+vectorize 1 \\(N/4/256\\) 256 8 4 vectorize 2 \\(N/4/256\\) 256 4 3 fuse 1 \\(N/256\\) 256 2 4 base 2 \\(N/256\\) 256 1 3 Pytorch 2 Unknown 256 4 Unknown <p>\u6574\u4f53\u6027\u80fd\u7684fundamental\u56e0\u7d20\uff1a\u8ba1\u7b97\u5e26\u5bbd\u548c\u8bbf\u5b58\u5e26\u5bbd</p>"},{"location":"2025/12/07/3-matrix-hadamard-product/#blockthread-level","title":"Block/Thread Level","text":"<p>\u5173\u4e8e\u8ba1\u7b97\uff0c\u8003\u8651\u4ee5\u4e0b\u51e0\u4e2a\u56e0\u7d20\uff1a \u8ba1\u7b97\u6570\u636e\u7c7b\u578b\uff1a\u90fd\u662ffloat \u8ba1\u7b97\u65b9\u5f0f\uff1a\u90fd\u662f\u4e58\u6cd5 \u8ba1\u7b97\u6b21\u6570\uff1afused kernel 2\u6b21\uff0cbase kernel 1\u6b21\uff1bvectorize kernel 4\u6b21\uff0cbase kernel 1\u6b21 \u8ba1\u7b97\u5f00\u9500\uff1a</p> <ul> <li>Fused &amp; Vectorized: \\(T_{compute} = 8~T_{\\times}\\)</li> <li>Vectorized: \\(T_{compute} = 4~T_{\\times}\\)</li> <li>Fused: \\(T_{compute} = 2~T_{\\times}\\)</li> <li>Base: \\(T_{compute} = T_{\\times}\\)</li> </ul> <p>\u5173\u4e8e\u8bbf\u5b58\uff1a\u5f71\u54cd\u56e0\u7d20\u9887\u591a\u3002 1. \u4e0d\u8003\u8651 L2 Cache (L2 Cache all miss)\uff0c\u53ea\u8003\u8651\u4ece global memory \u8bfb\u5199\uff1a</p> <ul> <li>Fused: \\(RQ_{mem} = 3 \\times RQ_{read} + RQ_{write} = 4 ~RQ_{r/w}\\)</li> <li> <p>Base: \\(RQ_{mem} = 2 \\times RQ_{read} + RQ_{write}=3~RQ_{r/w}\\)</p> </li> <li> <p>\u7ecf\u8fc7warm up\u540e\uff0c\u8003\u8651 L2 Cache (L2 Cache all hit)\uff0c\u5982\u679csize\u8db3\u591f\u5c0f\uff0c\u53ea\u8003\u8651\u4ece L2 Cache \u8bfb\u5199\uff1a(todo)</p> </li> </ul> <p>\u4ee5\u4e0a\u6211\u4eec\u7684\u5206\u6790\u9488\u5bf9\u7684\u662f\u5355\u4e2akernel\u4e2d\u7684\u5355\u4e2athread\u4e0a\uff0c\u73b0\u5728\u5f80\u4e0a\u5206\u6790\u4e00\u5c42\uff0c\u770b\u770b\u5355\u4e2akernel\u7684\u6574\u4f53\u7684\u8ba1\u7b97\u548c\u8bbf\u5b58\u5f00\u9500</p>"},{"location":"2025/12/07/3-matrix-hadamard-product/#kernel-level","title":"Kernel Level","text":"For Kernel kernel num Block num Thread_num Mul memory access fuse+vectorize 1 \\(N/1024\\) \\(N/4\\) \\(2N\\) \\(N\\) vectorize 2 \\(N/1024\\) \\(N/4\\) \\(N\\) \\(0.75N\\) fuse 1 \\(N/256\\) \\(N\\) \\(2N\\) \\(4N\\) base 2 \\(N/256\\) \\(N\\) \\(N\\) \\(3N\\) Pytorch 2 Unknown Unknown Unknown Unknown <p>\u5173\u4e8e\u8ba1\u7b97\uff1a</p> <p>~~   \u540c\u4e00kernel\u4e2d\u7684thread\u7684\u8ba1\u7b97\u662f\u5e76\u884c\u7684\uff0c\u4e0d\u5b58\u5728\u6570\u636e\u4f9d\u8d56\uff0c\u6211\u4eec\u8003\u8651\u5355\u4e2akernel\u542f\u52a8\u7684thread\u548cblock\u6570\u91cf\u3002\u53c2\u8003\u4e0a\u8868\uff0cvectorization\u7684kernel\u7684block\u548c\u5bf9\u5e94\u7684thread\u7684\u6570\u91cf\u51cf\u5c11\u5230\u539f\u6765\u76841/4\u3002\u5982\u679cGPU\u7684SM\u80fd\u591f\u5e76\u884c\u6267\u884c\u8fd9\u4e9bblocks\uff0c\u90a3\u4e48\u901f\u5ea6\u5e94\u8be5\u4e0d\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u6709warp\u7684\u6392\u961f\u5ef6\u8fdf\u3002\u4f46\u5355\u5c31\u8fd9\u4e00\u5c42\u6765\u8bf4\uff0c\u51cf\u5c11block\u7684\u6570\u91cf\u603b\u662f\u597d\u7684\u3002   ~~</p> <p>\u5728GPU\u4e2d\u8ba1\u7b97\u5ef6\u8fdf(Computing latency)\u901a\u5e38\u4e0d\u518d\u8003\u8651\uff0c\u4e3b\u8981\u8003\u8651\u8ba1\u7b97\u541e\u5410(Computing Throughput), \u7528CQ\u6765\u8868\u793a\u8ba1\u7b97\u541e\u5410\uff1a \\(CQ = \\frac{num(thread)}{num(SM)} \\times T_{compute}\\) \u62bd\u8c61\u51fa\u6765\u7b80\u5355\u7684\u500d\u6570\u5173\u7cfb\uff1a</p> <ul> <li>Fused &amp; Vectorized: \\(CQ = 2N~T_{\\times}\\)</li> <li>Vectorized: \\(CQ = N~T_{\\times}\\)</li> <li>Fused:  \\(CQ = 2N~T_{\\times}\\)</li> <li>Based: \\(CQ = N~T_{\\times}\\)</li> </ul> <p>\u5173\u4e8e\u8bbf\u5b58\uff1a\u5355\u4e2akernel\u6267\u884c\u7684\u8bbf\u5b58\u8bf7\u6c42\u6b21\u6570\u662f\u6709\u533a\u522b\u7684\uff0c\u53c2\u8003\u4e0a\u8868\uff0cfusion\u4f1a\u5c06\u8bbf\u5b58\u8bf7\u6c42\u589e\u957f1/3\uff0cvectorized kernel\u7684\u8bbf\u5b58\u8bf7\u6c42\u51cf\u5c113/4\uff0c\u662f\u5e26\u5bbd\u654f\u611f\u7684metrics\uff1a\u5982\u679c\u5e26\u5bbd\u591f\uff0c\u8fd9\u4e2a\u6570\u636e\u5c06\u4e0d\u4f1a\u5f71\u54cdkernel\u7684\u6267\u884c\u901f\u5ea6\uff0c\u5426\u5219\u8fd9\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u51cf\u5c11\u8bbf\u5b58\u8bf7\u6c42\u6b21\u6570\u603b\u662f\u597d\u7684\u3002</p> <ul> <li>Fused &amp; Vectorized: \\(RQ_{mem} = N \\times RQ_{r/w}\\)</li> <li>Vectorized: \\(RQ_{mem} = 0.75N \\times RQ_{r/w}\\)</li> <li>Fused: \\(RQ_{mem} = 4N \\times RQ_{r/w}\\)</li> <li>Based: \\(RQ_{mem} = 3N \\times RQ_{r/w}\\)</li> </ul>"},{"location":"2025/12/07/3-matrix-hadamard-product/#consider-all-together","title":"Consider All together","text":"<p>Notably, the unfused approach introduces a data dependency between the successive kernels\uff1a </p> <ul> <li>Fused: \\(T_{total} = T_{kernel}\\)</li> <li>Non-fused: \\(T_{total} = 2~T_{kernel}\\)</li> </ul> <p>\u8003\u8651 \\(T_{kernel}\\), \u6211\u4eec\u5047\u8bbe\u8ba1\u7b97\u80fd\u591f\u5b8c\u5168\u5e76\u884c\uff08\u5927\u6982\u7387\u662f\uff09\uff0c\u4e14\u6240\u6709\u8bbf\u5b58\u8bf7\u6c42\u7686\u53d1\u9001\u5230 global memory\uff0c\u90a3\u4e48\uff1a $$ T_{kernel} = T_{compute} + T_{mem},\\ $$ $$ T_{compute} =\\frac{ CQ}{CQ(\\times)}\\ $$</p> <p>$$ T_{mem} = \\frac{RQ_{mem} \\times 4\\text{Bytes}}{BW(global)} $$ where \\(T_{sm}\\) is the SM instruction transmission cycle, and \\(BW_g\\) is the global memory bandwidth. translate to the formula using \\(CQ(\\times), RQ_{global},N\\): </p> <ul> <li>Fused &amp; Vectorized: \\(T_{total} = (\\frac{2}{CQ(\\times)} + \\frac{4}{BW(global)})~N\\)</li> <li>Vectorized: \\(T_{total} = 2 \\times (\\frac{1}{CQ(\\times)} + \\frac{3}{BW(global)})~N\\)</li> <li>Fused: \\(T_{total} = (\\frac{2}{CQ(\\times)} + \\frac{16}{BW(global)}) ~N\\)</li> <li>Based: \\(T_{total} =2 \\times (\\frac{1}{CQ(\\times)} + \\frac{12}{BW(global)})~N\\)</li> </ul> <p>\u5f88\u663e\u7136\uff0c\u8ba1\u7b97\u5f00\u9500\u5e76\u4e0d\u4f1a\u968f\u7740kernel\u7684\u878d\u5408\u4e0e\u5426\u800c\u53d8\u5316\uff0c\u540c\u65f6\uff0ckernel\u7684\u5207\u6362\u4e5f\u5b58\u5728\u4e00\u5b9a\u7684\u5f00\u9500\uff0c\u6b64\u5916\u5185\u5b58\u4f53\u7cfb\u7684\u5f71\u54cd\u4e5f\u4e0d\u53ef\u5ffd\u7565\uff0c\u5728\u8fd9\u91cc\u6211\u4eec\u9ed8\u8ba4\u4e86\u4eceglobal memory\u8bfb\u53d6\uff0c\u6240\u4ee5\u51b3\u5b9a\u505a\u4e00\u4e2aN\u4e0d\u65adscale\u7684speedup\u7684\u5b9e\u9a8c\u56fe:</p>"},{"location":"2025/12/07/3-matrix-hadamard-product/#configuration","title":"Configuration","text":"<p>GPU: 1x RTX4090, L2 Cache Size = 72MB \\(N\\): matrix size, \\(N\\times N\\) </p> <p>\\(N &lt; 1600\\): </p> <p>Q: \u4e3a\u4ec0\u4e48Fused\u7684\u4f18\u5316\u4f4e\u4e8e1.5\uff1f</p> <p>A: cuda-extension\u5b58\u5728launch\u5f00\u9500</p> <p>Q: \u4e3a\u4ec0\u4e48Vectorization\u6ca1\u6709\u4f18\u5316\u6548\u679c\uff1f</p> <p>A: \u5185\u5b58\u5e26\u5bbd\u6ca1\u6709\u6253\u6ee1</p> <p>$ 1600 &lt; N &lt; 2200$:</p> <p>Q: \u4e3a\u4ec0\u4e48\u6027\u80fd\u6709\u6025\u5267\u63d0\u5347\uff1f</p> <p>A: L2 Cache cover\u4e86\u6240\u6709\u7684\u6570\u636e\uff0c$2200 \\times 2200 \\times 4 \\times 4B \\approx 72MB $</p> <p>$ 2200 &lt; N &lt; 3200$:</p> <p>Q: \u6027\u80fd\u5760\u843d &amp; \u91cd\u65b0\u722c\u8d77\uff1f A: TODO</p> <p>\\(N &gt; 3200\\):</p> <p>\u6570\u636e\u91cf\u8fbe\u5230\u4e86\u6211\u4eec\u7684\u5206\u6790\u5047\u8bbe\u7684\u60c5\u51b5\uff0c\u6240\u6709Memory Access\u540c\u65f6\u8bbf\u95eeGlobal Memory\uff0c\u540c\u65f6\u8ba1\u7b97 &amp; \u542f\u52a8\u5f00\u9500\u5ffd\u7565\u4e0d\u8ba1\uff0c\u662f\u4e00\u4e2apromising\u7684\u7ed3\u679c</p> <p>Overall: </p> <p>Q: \u4e3a\u4ec0\u4e48\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\uff0cvectorization\u7684\u6548\u679c\u4e0d\u660e\u663e\uff1f A: Gemini: \u56e0\u4e3aGPU\u786c\u4ef6\u4f1a\u81ea\u52a8\u8fdb\u884c\u8bbf\u5b58\u878d\u5408 \\((memory~coalescing)\\)</p>"},{"location":"2025/12/16/3mm/","title":"3mm","text":"<p>3 Matrix Multiplication \u4f18\u5316\uff0c\u505a\u5230PyTorch\u539f\u751f\u5b9e\u73b0\u76843\u500d</p> <p>\u9996\u5148\u770b\u770bbaseline\u7684\u6548\u679c\uff0c\u4ee3\u7801\uff1a</p> <pre><code>import torch\nimport torch.profiler\n\n# \u51c6\u5907\u6570\u636e\ndevice = \"cuda\"\na = torch.randn(512, 512, device=device)\nb = torch.randn(512, 512, device=device)\nc = torch.randn(512, 512, device=device)\n# \u9884\u70ed\nfor _ in range(10):\n    d = a @ b @ c\n\n# Profile\nwith torch.profiler.profile(\n    activities=[torch.profiler.ProfilerActivity.CUDA],\n    record_shapes=True,\n) as prof:\n    d = a @ b @ c\n\n# \u6253\u5370\u7ed3\u679c\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n</code></pre> <p>\u770b\u5230PyTorch\u7528\u7684kernel\u662f<code>ampere_sgemm_32x32_sliced1x4_nn</code>\uff0c\u5e76\u8c03\u75282\u6b21:</p> <p>  - <code>sgemm</code>: single float gemm  - <code>32x32</code>: \u4e00\u4e2a CTA \u8d1f\u8d23\u7684\u5b50\u5757\u5927\u5c0f  - <code>sliced1x4</code>: \u5bf9K\u7ef4\u505a4\u8def\u5207\u5206  - <code>nn</code>: \u8f93\u5165\u77e9\u9635\u5728\u5185\u5b58\u4e0d\u505a\u91cd\u6392 \u5f88\u660e\u663e\uff0c\u53ef\u4ee5\u505akernel fusion\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u5206\u5757\u7b56\u7565\u3002 \u63a8\u5bfc\u4e00\u4e0b3MM\u7684\u8ba1\u7b97\u5f3a\u5ea6: $$ A \\times B \\times C = E $$ where \\(A, B, C\\) are all \\(N \\times N\\).</p> <p>For $ e_{ij}$: $$ e_{ij} = \\sum_{r=1}^N(\\sum_{s=1}^N (a_{is}b_{sr})c_{rj}) $$ Multiple Operation: $ N^3 $ Add Operation: $ N^2+N-2 $ Read Bytes: \\(3N^2 \\times 4~\\text{Bytes}\\) Write Bytes: \\(N^2 \\times 4~\\text{Bytes}\\) Arithmetic Intensity: $ \\frac{N^3 + N^2 + N - 2}{4N^2} \\approx \\frac{N}{16} $ \u4e5f\u5c31\u662f\u8bf4\uff0c\u8ba1\u7b97\u5f3a\u5ea6\u968f\u7740N\u7684\u6269\u5927\u800c\u6269\u5927\uff0c\u5b58\u5728memory bound\u548ccompute bound\u4e4b\u95f4\u7684\u53d8\u5316\u3002</p> <p>\u5148\u5199\u4e2anaive kernel, \u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23E\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\u7684\u8ba1\u7b97\uff0c\u6bcf\u4e2ablock\u8d1f\u8d23E\u4e2d\u4e00\u884c\u7684\u8ba1\u7b97\uff0c\u8fd9\u6837block\u5185\u90e8\u53ef\u4ee5\u7528shared memory\u5171\u4eab \\(A \\times B\\) \u7684\u4e2d\u95f4\u7ed3\u679c\uff1a</p> <pre><code>#define OFFSET(r, c, ld) ((r) * (ld) + (c))\n// \u7b2c\u4e00\u4e2a\u7248\u672c\uff1a\n// Thread: \u8d1f\u8d23out\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\n// Block: \u8d1f\u8d23out\u4e2d\u7684\u4e00\u884c\n// Shared Memory: \u5b58\u50a8 A*B \u7684\u4e00\u884c\u4e2d\u95f4\u7ed3\u679c\uff0c\u4e3a\u540e\u7eed\u6bcf\u4e2aThread\u8ba1\u7b97 Out \u5143\u7d20\u670d\u52a1\n__global__ void mul_tri_kernel_naive(float* a, float* b, float* c, float* out, int n) {\n    extern __shared__ float s_temp[];\n    int col = threadIdx.x;\n    int row = blockIdx.x;\n    // \u6b65\u9aa4 1: \u534f\u4f5c\u8ba1\u7b97 (A x B) \u7684\u7b2c row \u884c\n    if (col &lt; n) {\n        float val = 0.0f;\n        for (int k = 0; k &lt; n; k++) {\n            val += a[OFFSET(row, k, n)] * b[OFFSET(k, col, n)];\n        }\n        s_temp[col] = val; // \u5199\u5230 shared memory\n    }\n        __syncthreads(); \n    // \u6b65\u9aa4 2: \u7528(A x B) \u7684\u7b2c row \u884c\u7b97\u51fa E \u7684\u7b2c row \u884c\n    if (col &lt; n &amp;&amp; row &lt; n) {\n        float psum = 0.0f;\n        for (int k = 0; k &lt; n; k++) {\n            psum += s_temp[k] * c[OFFSET(k, col, n)];\n        }\n        out[OFFSET(row, col, n)] = psum;\n    }\n}\n\nvoid launch_mul_tri_naive(float* a, float* b, float* c, float* out, int n) {\n    //patriton configuration here\n    dim3 threads(n); \n    dim3 blocks(n);\n    mul_tri_kernel_naive&lt;&lt;&lt;blocks, threads, n * sizeof(float)&gt;&gt;&gt;(a, b, c, out, n);\n}\n</code></pre> <p>Gemini\u8bc4\u4ef7\uff1a</p> <ul> <li>\u603b\u7ed3\u4e0e\u8bc4\u5206\u903b\u8f91\u6b63\u786e\u6027\uff1a100/100</li> <li>\u5de5\u7a0b\u9c81\u68d2\u6027\uff1a50/100 (\u53d7\u9650\u4e8e \\(N \\le 1024\\))</li> <li>\u8bed\u6cd5\u6b63\u786e\u6027\uff1a80/100 (<code>const</code>\u7c7b\u578b\u4e0d\u5339\u914d, \u8fd9\u91cc\u4ee3\u7801\u5df2\u4fee\u6539)</li> <li>\u7ed3\u8bba\uff1a\u53ea\u8981\u4fdd\u8bc1\u6d4b\u8bd5\u65f6\u7684<code>n</code>\u4e0d\u8d85\u8fc7 1024\uff0c\u8fd9\u6bb5\u4ee3\u7801\u5b8c\u5168\u53ef\u4ee5\u8dd1\u901a\uff0c\u4e14\u7ed3\u679c\u6b63\u786e\u3002</li> </ul>"},{"location":"2025/12/07/cuda-programming-guide-1/","title":"CUDA Programming Guide-1","text":"<p>\u5bfc\u8a00</p> <p>CUDA is a parallel computing platform and programming model that enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). Wishing to learn more about CUDA, this guide is a great place to start.</p> <p>Source link: CUDA Programming Guide</p> <p>The guide is broke down into 5 primary parts: </p> <p>Part 1: Introduction and Programming Model Abstract </p> <p>Part 2: Programming GPUs in CUDA</p> <p>Part 3: Advanced CUDA</p> <p>Part 4: CUDA Features</p> <p>Part 5: Technical Appendices</p> <p>Parts 1-3 provide a guided learning experience for developers new to CUDA, though they also provide insight and updated information useful for CUDA developers of any experience level.</p> <p>Parts 4 and 5 provide a wealth of information about specific features and detailed topics, and are intended to provide a curated, well-organized reference for developers neeing to know more details as they write CUDA applications.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#part-1-introduction-and-programming-model-abstract","title":"Part 1: Introduction and Programming Model Abstract","text":""},{"location":"2025/12/07/cuda-programming-guide-1/#11-intro","title":"1.1 Intro","text":"<p>A GPU provides much higher instruction throughput and memory bandwidth than a CPU within a similar price and power envelope.</p> <p>Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs.</p> <p></p> <p>Specialized library: cuBLAS, cuFFT, cuDNN, CUTLASS</p> <p>higher-level language: Triton, tilelang ...</p> <p>The NVIDIA Accelerated Computing Hub contains resources, examples, and tutorials to teach GPU and CUDA computing.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#12-programming-model","title":"1.2 Programming Model","text":""},{"location":"2025/12/07/cuda-programming-guide-1/#121-heterogeneous-system","title":"1.2.1. Heterogeneous System","text":"<p>Device code, kernel, kernel launch</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#122-gpu-hw-model","title":"1.2.2. GPU HW Model","text":"<p>GPU = A collection of Streaming Multiprocessors (SMs)</p> <p>GPC = Graphics Processing Clusters = a group of SMs</p> <p>SM = 1 local Reg File + 1 Uni-Data Cache + multi Function Units</p> <p>Uni-Data Cache = shared memory + L1C, software configurable</p> <p></p>"},{"location":"2025/12/07/cuda-programming-guide-1/#1221-thread-block-grid","title":"1.2.2.1 Thread block &amp; grid","text":"<p>block: a set of threads</p> <p>grid: block organization, the blocks are the same size.</p> <p>dimension: blocks and grids include the concept of dimension.</p> <p>in kernel launch, a execution configuration specifies the grid an block dimensions, also the SM, stream and Cluster related params.</p> <p>every thread has own id, determined by the block location, grid location. And the thread could know the size of blocks and grids.</p> <p>A block of threads are running in a single SM.</p> <p>No guarantee of scheduling between thread blocks, so no data dependency between thread blocks</p> <p></p>"},{"location":"2025/12/07/cuda-programming-guide-1/#thread-block-clusters","title":"Thread Block Clusters","text":"<p>a group of thread blocks, organized by adjacent blocks in a grid, offers some synchronization and communications</p> <p>Specifically, A cluster is executed in a single GPC</p> <p>Threads in different block but in the same cluster can sync &amp; comm by Cooperative Groups</p> <p>Now assume T is Thread, C is Cluster, B is block, G is Grid, </p> <p>Ts in a C but different B can communicate and synchronize by interfaces provided by Cooperative Groups and access their shared memory -- distributed shared memory.</p> <p>cluster size is determined by HW of GPU.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#warp-simt","title":"Warp &amp; SIMT","text":"<p>In a B, Ts are organized into groups of 32 Ts called Warp. Now W is Warp.</p> <p>Ts in W execute the same codes by Single-Instruction Multiple-Threads (SIMT)</p> <p>T has a W id, 0-31. </p> <p>The way that Ts into W is predictable (?) cited Hardware Multithreading.</p> <p>Control flow branch may causes some Ts in W masked off, as the fig follows.</p> <p></p> <p>CUDA cannot see Warp.</p> <p>helpful for  global memory coalescing and shared memory bank access patterns.</p> <p>The num of Ts in a B better are a multiple of 32 (Ts size in a W).</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#123-gpu-memory","title":"1.2.3 GPU Memory","text":""},{"location":"2025/12/07/cuda-programming-guide-1/#1231-dram-memory-in-heterogeneous-systems","title":"1.2.3.1 DRAM Memory in Heterogeneous Systems","text":"<p>System Memory = Host Memory = CPU DRAM</p> <p>GPU DRAM = Global Memory, accessible to all SMs.</p> <p>Virtual Memory in GPU: can recognize which GPU, whether in GPU memory or CPU memory</p> <p>There are lots of API for memory control.</p> <p>CUDA can use Unified Memory -- a mechanism, to automatically handling the placement of memory in the runtime.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#1232-gpu-on-chip-memory","title":"1.2.3.2 GPU on-chip Memory","text":"<p>Reg files (for temp variables) + shared memory (can be used for data exchange in Block &amp; Cluster)</p> <p>on-chip Memory is shared by all Ts in a B.</p> <p>However, the Reg files is allocated to Ts while the Shared memory is allocated to Bs.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#caches","title":"Caches","text":"<p>L1 is from shared memory, L2 is for all SMs in GPU, constant cache is for constant variables in global memory.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#1233-unified-memory","title":"1.2.3.3 Unified Memory","text":"<p>Section Unified Memory introduces the different categories of unified memory systems. Section Unified Memory contains many more details about use and behavior of unified memory in all situations.</p> <p>CPU codes only access CPU memory while GPU kernels only access GPU memory in addition to mapped memory.</p> <p>CUDA APIs work for the data copies between them.</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#13-the-cuda-platform","title":"1.3 The CUDA Platform","text":""},{"location":"2025/12/07/cuda-programming-guide-1/#131-compute-capability-cc-sm-versions","title":"1.3.1. Compute Capability (CC)  &amp; SM Versions","text":"<p>GPU has a CC number documented in the  Section 5.1 appendix, looking like X.Y</p> <p>E.g: CC 12.0 - sm_120</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#132-cuda-toolkit-nv-driver","title":"1.3.2. CUDA Toolkit &amp; NV Driver","text":"<p>Nvidia Driver = GPU OS</p> <p>CUDA Toolkit = Lib + Headers + Tools</p> <p>CUDA runtime = one Library of CUDA Toolkit</p> <p>About Compatibility: The CUDA Compatibility document provides full details of compatibility between different GPUs, NVIDIA Drivers, and CUDA Toolkit versions.</p> <p>Driver API covers the CUDA Runtime API</p> <p>The full API reference for the CUDA runtime API functions can be found in the CUDA Runtime API Documentation .</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#133-parallel-thread-execution-ptx","title":"1.3.3. Parallel Thread Execution (PTX)","text":"<p>PTX is an invisible virtual instruction set architecture, which aligns with SM Versions.</p> <p>Full documentation on PTX can be found in the PTX ISA .</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#134-cubins-fatbins","title":"1.3.4 Cubins &amp; Fatbins","text":"<p>Cubin: CUDA binary for physical GPU.</p> <p>Fatbin: a container of mutiple versions of cubins &amp; PTX</p> <p></p>"},{"location":"2025/12/07/cuda-programming-guide-1/#1341-binary-compatibility","title":"1.3.4.1. Binary Compatibility","text":"<ol> <li> <p>a cubin's version is CC8.6, then GPU with 8.x where x &gt;= 6 can execute while GPU with 8.x where x &lt; 6 cannot.</p> </li> <li> <p>Where X.Y, different X is not coampatible.</p> </li> </ol>"},{"location":"2025/12/07/cuda-programming-guide-1/#1342-ptx-compatibility","title":"1.3.4.2. PTX Compatibility","text":"<p>In fatbins, if PTX is compute_80, it can be JIT compiled at application runtime for any CC equal or higher to the CC of the PTX code (such as sm_120).</p>"},{"location":"2025/12/07/cuda-programming-guide-1/#1343-just-in-time-compilation","title":"1.3.4.3. Just-in-Time Compilation","text":"<p>NVRTC can be used to compile CUDA C++ device code to PTX at runtime.</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/","title":"Some Notes about Transformer in LLM","text":"<p>\u6700\u8fd1\u60f3\u5165\u95e8\u4e00\u4e0b\u5927\u6a21\u578b\u7684serving\u548cinference\u3002\u53ef\u80fdTransformer\u7684\u67b6\u6784\u662f\u4e00\u4e2a\u597d\u7684\u5207\u5165\u70b9\u3002</p> <p>\u7b2c\u4e00\u6b65\uff0ctransformer\u9700\u8981\u5c06\u8f93\u5165\u7684prompt\u53d8\u6210input embeddings</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#1-tokenizer","title":"1. Tokenizer","text":"<p>word embedding uses various methods (tiktoken in Llama), while the position emdedding is different.</p> <p>in original Transformer, </p> \\[ PE{(t, 2i)} = \\sin\\left(\\frac{t}{10000^{2i/d{\\text{model}}}}\\right) \\] \\[ PE{(t, 2i+1)} = \\cos\\left(\\frac{t}{10000^{2i/d{\\text{model}}}}\\right) \\] <p>, where \\(i\\) is the dimension position in a token, \\(t\\) is the token's absolute position in the query.</p> <p>transformer takes the sum of \\(\\mathbf{p}_t\\) (Word Embedding) and  \\(\\mathbf{e}_t\\) (Word Embedding) as the input \\(\\mathbf{z}_t\\): </p> \\[ \\mathbf{z}_t = \\mathbf{e}_t + \\mathbf{p}_t \\]"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#optimizations","title":"Optimizations","text":""},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#rope","title":"RoPE","text":"<p>Llama uses another kind of Position Embedding, RoPE to describe the postion, and it works on the \\(Q \\&amp; K\\) caculation by rotation not the input generation by addition.</p> <p>For a token at the absolute position \\(m\\), the rotation matrix \\(R_m\\), it will rotate every pairs \\((q_1, q_2)\\) in \\(Q_m\\). the number of the pairs is \\(d_{model}/2\\), </p> \\[ Q'_m = R_mQ_m \\] <p>where every pair \\((q_1, q_2)\\) is rotated by \\(\\theta_i\\), </p> \\[ \\begin{pmatrix} q'_1 \\\\ q'_2 \\end{pmatrix} =  \\begin{pmatrix} \\cos m\\theta &amp; -\\sin m\\theta \\\\ \\sin m\\theta &amp; \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} q_1 \\\\ q_2 \\end{pmatrix} \\] <p>Where the \\(\\theta\\) is calculated by \\(i\\) and \\(d_{model}\\), \\(\\theta_i = \\frac{1}{b^{2i / d}}, b = 10000\\)</p> <p>Notice that there is no additional computation compared with original implementation (easy to prove).</p> <p>Now look into the implementation in Llama: </p> <pre><code>def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\n</code></pre>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#2-q-k-v-calculation","title":"2. Q, K, V Calculation","text":"<p>However we have got the input \\(X\\) (shown as vector \\(z\\) in Sec 1). The shape of \\(X\\) can be defined by 2 parameters:</p> <ul> <li>\\(L\\): the length of the sequence that \\(X\\) represents.</li> <li>\\(D\\): the dimension of each token in the sequence.</li> </ul> <p>We define \\(X\\) as:  $$ X \\in \\mathbb{R}^{L \\times d_{\\text{model}}} $$</p> <p>Now we compute the Queries, Keys and Values: In the self-attention mechanism, we compute the Queries, Keys and Values using 3 weight matrixes:</p> <p>$$ W^Q \\in \\mathbb{R}^{L \\times d_{\\text{k}}},\\ W^K \\in \\mathbb{R}^{L \\times d_{\\text{k}}},\\ W^V \\in \\mathbb{R}^{L \\times d_{\\text{v}}} $$ X will be projected into 3 subspaces to ge Q, K and V:</p> \\[ Q = XW^Q,\\\\ K = XW^K,\\\\ V = XW^V \\] <p>note: Q, K, V always have the same dimension \\(d\\).</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#quantization","title":"Quantization","text":""},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#basic-principle","title":"Basic Principle","text":"<p>The usage of these \"\\(W\\)\" occupies much memory bandwidth due to their data type being FP16 or FP32. So one optimization is to quantize them into integer(INT8 or INT4).</p> <p>The quantization is done by: $$ W_{quant} = \\text{clip}\\left(\\text{round}\\left(\\frac{W_{fp}}{S} + Z\\right), min, max\\right) $$ \\(W_{fp}\\): The original high-precision weight (e.g., 16-bit float).</p> <p>\\(S\\) (Scale): A floating-point factor that determines the step size of the quantization buckets.</p> <p>\\(Z\\) (Zero Point): An integer that ensures the real value zero is exactly representable (crucial for sparsity).</p> <p>\\(W_{quant}\\): The resulting integer index (e.g., a 4-bit integer ranging from 0 to 15, or -8 to 7).</p> <p>During inference, we will dequantize the quantized weight \\(W_{quant}\\) back to the original floating-point weight \\(W_{fp}\\) using the following formula:</p> \\[ W'_{fp} = S \\times (W_{quant} - Z) \\]"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#floating-point-family","title":"Floating Point Family","text":"<p>Used primarily for Training and High-Precision Inference. | Data Type | Bit Width | Structure | Primary Use Case | Key Characteristics | | :---: | :---: | :---: | :---: | :---: | | FP32 (Single Precision) | 32-bit | 1 Sign, 8 Exp, 23 Mantissa | Master Weights | The \"Gold Standard\" for accuracy. Used to store master weights during training to prevent underflow. | | FP16 (Half Precision) | 16-bit | 1 Sign, 5 Exp, 10 Mantissa | Legacy Inference | Standard on older GPUs (Volta/Turing). Risk: Prone to overflow (values &gt; 65,504 turn to NaN) during training. | | BF16 (Brain Float 16) | 16-bit | 1 Sign, 8 Exp, 7 Mantissa | Standard Training | Keeps the same dynamic range (Exponent) as FP32 but truncates precision. Much more stable than FP16 for training. | | FP8 (E4M3) | 8-bit | 1 Sign, 4 Exp, 3 Mantissa | Inference/Weights | Higher precision relative to range. Optimized for storing weights and activations in H100+ GPUs. | | FP8 (E5M2) | 8-bit | 1 Sign, 5 Exp, 2 Mantissa | Training Gradients | Higher dynamic range (equivalent to FP16). Optimized for gradients which vary wildly in magnitude. |</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#integer-family","title":"Integer Family","text":"<p>Used primarily for Quantized Inference to save memory and bandwidth.</p> Data Type Bit Width Range Primary Use Case Key Characteristics INT8 8-bit -128 to 127 Production Inference The industry standard for \"lossless-feeling\" quantization. Supported natively by Tensor Cores. INT4 4-bit -8 to 7 Weight Storage The current \"sweet spot\" for LLMs. Weights are stored in INT4 and dequantized on-the-fly for computation. INT1 1-bit 0 or 1 Experimental Used in \"1-bit LLMs\" (e.g., BitNet). Weights are effectively ternary \\(\\{-1, 0, 1\\}\\). Extremely efficient but requires specialized training. <p>The accuracy loss seems apparently great. But the memory savings is huge ( \\(2-4\\times\\))</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#3-scaled-dot-product-attention","title":"3. Scaled Dot-Product Attention","text":""},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#310-reshape-for-multi-head-attention","title":"3.1.0. Reshape for Multi-head Attention","text":"<p>The shape of Q and K is \\(L \\times d_k\\). Now we reshape the Q and k to \\(L \\times h \\times d_h\\), where $ d_k = d_h \\times h$.</p> <p>Note that \\(d_h\\) is the hidden dimension within every token in the \\(L\\) sequence.</p> <p>We want the computation of \\(QK^T\\) to be more parallelizable and efficient. </p> <p>So we reshape the Q and K to \\(h \\times L \\times d_h\\) and \\(h \\times L \\times d_h\\), respectively. No the shape of Q and K is <code>[h, L, d_h]</code> (no considering Batch Size).</p> <p>Now we can ensure the \\(d_h\\) at fixed size: \\(128 /256\\). and parallelize the computation of \\(QK^T\\) between the \\(h\\) heads.</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#311-attention-calculation","title":"3.1.1. Attention Calculation","text":"<p>The score is calculated as follows: $\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_h}} + M\\right)V\\)$ In every head, the shape of \\(QK^T\\) is <code>[L, L]</code>, and the shape of \\(V\\) is <code>[L, d_h]</code>.  Notably the \\(d_v\\) is always equal to \\(d_k\\). So now we use \\(d\\) to unify \\(d_v\\) and \\(d_k\\).</p> <p>For MHA:</p> <p>The V will be divided to <code>[h, L, d_h]</code> align with the Q and K. And the Attention Weights will be multiplied with a corresponding V in the same head.</p> <p>For GQA:</p> <p>To optimize the memory consumption of KV cache. GQA choose to store less heads of  \\(K\\) &amp; \\(V\\) and distribute them to different groups of attention weights</p> <p>For example, if we have 8 heads, and we want to distribute them to 2 groups. Then we will have 4 heads in each group. Instead of storing 8 heads of \\(KV\\), we will store 2 heads of \\(KV\\) for 2 groups and 4 heads of \\(Q\\) in each group.</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#312-head-concatenation","title":"3.1.2. Head Concatenation","text":"<p>Now we have \\(H\\) heads, each head \\(i\\) has an attention matrix \\(Z_i\\) of shape: <code>[L, d_h]</code></p> <p>We concatenate all heads along the last dimension \\(d_h\\) into a single matrix \\(Z\\) of shape: <code>[L, d]</code>, where \\(d = d_h \\times H\\):</p> <p>$$ Z = \\text{Concat}(Z_1, Z_2, \\dots, Z_h) $$ Now then apply a output linear projection to \\(Z\\):</p> \\[ \\text{Attention} = Z_{concat} \\times W^O \\]"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#4-add-normalization","title":"4. Add &amp; Normalization","text":""},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#41-residual-connection","title":"4.1. Residual Connection","text":"<p>Execute Residual Connection (Add) and Layer Normalization: </p> \\[X = X_{input} + \\text{Attention}(X_{input})\\]"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#42-normalization","title":"4.2. Normalization","text":"<p>Here are 2 ways to normalize:</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#421-layer-normalization","title":"4.2.1 Layer Normalization","text":"<p>For \\(x \\in X\\), \\(x\\) of shape<code>[d, 1]</code> is normalized by:</p> <p>$$ \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$ where \\(\\mu\\) is the mean of \\(x\\) and \\(\\sigma^2\\) is the variance of \\(x\\).</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#422-root-mean-square-rms-normalization","title":"4.2.2 Root Mean Square (RMS) Normalization","text":"\\[\\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}\\] <p>Note</p> <p>Originally, Normalization was done after Attention Calculation like \\(Attention \\rightarrow Add \\rightarrow Norm\\). However, Nowadays, Normalization(RMS Norm) is done before Attention like \\(Norm \\rightarrow Attention \\rightarrow Add\\). The same thing will happen in FFN blocks.</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#5-ffn-mlp","title":"5. FFN / MLP","text":"<p>The defining characteristic of this block is that it is applied independently to every single token position.</p> <ul> <li>The same matrix weights (\\(W_{up/down}, b\\)) are used for every token.</li> <li>Token \\(i\\) does not interact with Token \\(j\\) inside this block.</li> </ul> <p>Mathematically, if the input is a matrix \\(X \\in \\mathbb{R}^{L \\times d}\\), the FFN operates on each row \\(x_i\\) identically.</p> <p>The fundamental structure involves projecting the input vector into a higher-dimensional (\\(d_{ff}\\)) space (Expansion), applying a non-linearity, and projecting it back (Contraction).</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#51-standard-ffn","title":"5.1. Standard FFN","text":"<p>$$ \\text{FFN}(x) = \\text{Activation}(x W_{up} + b_1) W_{down} + b_2 $$ where the \\(d_{ff}\\) is usually the \\(4 \\times d\\)</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#52-gated-ffn","title":"5.2. Gated FFN","text":"\\[ \\text{FFN}_{\\text{SwiGLU}}(x) = (\\text{SiLU}(x W_{gate}) \\odot (x W_{up})) W_{down} $$ where the $d_{ff}$ is usually the $2.6 \\times d$, because there are **1 more projection weight matrixes**, engineers want to keep the number of parameters  align with the standard FFN. ### 5.3. Activation #### ReLU Rectified Linear Unit: $$ f(x) = max(0,x) $$ #### GELU Gaussian Error Linear Unit: $$ f(x) = x \\cdot \\Phi(x) $$ Where $\\Phi(x)$ is the Cumulative Distribution Function (CDF) of the Standard Normal Distribution: $$ \\Phi(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right] $$ Approximation (Used in practice for speed): $$ f(x) \\approx 0.5x \\left( 1 + \\tanh \\left[ \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715 x^3 \\right) \\right] \\right) \\]"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#silu","title":"SiLU","text":"<p>Swish-Gated Linear Unit: $$ f(x) = x \\cdot \\sigma(x) $$ Where \\(\\sigma(x)\\) is the Sigmoid function: $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#why-silu-in-gated-ffn","title":"Why SiLU in Gated FFN?","text":"<p>It is the \"Goldilocks\" function\u2014smooth, non-linear, and computationally efficient enough to be multiplied across billions of parameters.</p> <p>After FFN, we apply another Add and then enter the next iteration of Attention Block:</p> \\[ x_{output} = x_{mid} + \\text{FFN}( \\text{RMSNorm}(x_{mid}) ) \\] <p>Note</p> <p>the normalization is applied before the FFN in modern transformers, while it is applied after in the original transformer paper, which is aligned with the Attention Block.</p>"},{"location":"2025/11/15/some-notes-about-transformer-in-llm/#summary","title":"Summary","text":"<p>For decode-only transformer, here are 3 key blocks:</p> <ol> <li>token embedding</li> <li>attention block</li> <li>feed forward network block</li> </ol> <p>To be more specific, it consists of many kernels with variety of weight matrixes.  They are full of <code>gemm</code>  and <code>norm &amp; add</code> operations, where much optimization can be done.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving","text":"<p>\u4ece\u8fd9\u7bc7\u6587\u7ae0\u51fa\u53d1\u4e86\u89e3\u4e00\u4e0b\u6a21\u578b\u91cf\u5316\u6280\u672f\uff1aQServe</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#background-quantization","title":"Background-Quantization","text":"<p>Quantization makes some parameters in LLM smaller to optimize the memory usage. the process of quantization is: </p> \\[ \\mathbf{Q}_{\\mathbf{X}} = \\left\\lceil \\frac{\\mathbf{X}}{s} + z \\right\\rfloor, \\quad  s = \\frac{\\mathbf{X}_{\\max} - \\mathbf{X}_{\\min}}{q_{\\max} - q_{\\min}}, \\quad  z = \\left\\lceil q_{\\min} - \\frac{\\mathbf{X}_{\\min}}{s} \\right\\rfloor \\]"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#motivation","title":"Motivation","text":"<p>Existing methods of quantization can be divided into 3 categories:  - W4A16 (per-group quantization)  - W8A8 (per-channel for W and per-token for A)  - W4A4 (per-group quantization)</p> <p>Quote</p> <p>Weight and KV cache quantization (e.g. W4, KV4) can reduce the memory footprint in LLM serving. Quantizing both weight and activation (e.g. W8A8) can also improve the peak computation throughput. Choosing the right precision for LLM deployment is a difficult task. Existing solutions can be divided into three categories: W4A16 (pergroup), W8A8 (per-channel weight + per-token activation), W4A4 (per-group). We will demonstrate in this section why W4A8KV4 is a superior choice.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#why-w4a8kv4","title":"Why W4A8KV4?","text":"<p>The experiments in this paper are evaluated on the A100:  W4A8KV4:  1. higher throughput (Compared to W8A8 &amp; FP16 &amp; W4A16) 2. higher Bandwidth (Compared to W4A8 withou KV4) 3. lower accuracy loss (Compared to W4A4) 4. without CUDA core's limitation (Compared to W4A4 in Atom &amp; W4A16 in TRTLLM)</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#qoq","title":"QoQ","text":""},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#progressive-group-quantization","title":"Progressive Group Quantization","text":"<p>We first apply per-channel symmetric <code>INT8</code> quantization: </p> \\[ \\hat{\\mathbf{W}}=\\mathbf{Q}_{{\\mathbf{W}_{s8}}}^{(0)}\\cdot\\mathbf{s}_{{\\mathrm{fp}16}}^{(0)}, \\] <p>Then, we then employ per-group asymmetric <code>INT4</code> quantization: $$ \\mathbf{Q_{W_{s8}}}^{(0)}=(\\mathbf{Q_{W_{u4}}}-\\mathbf{z_{u4}})\\cdot\\mathbf{s_{u8}}^{(1)},$$</p> <p>where \\(\\mathbf{s_{u8}}^{(1) \\in \\mathbb{R}^{n \\times k/g}}\\) is the unsigned 8-bit group-wise quantization scales.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#protective-quantization-range","title":"Protective Quantization Range","text":"<p>Naively applying these 2 quantization will have out-of-bounds risks. (Easy to be proved. ) Therefore, we shrink the <code>INT8</code> symmetric range from [-127, 127] to [-119, 119].</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#compared-to-previous-two-level-quantization","title":"Compared to previous two-level quantization","text":"<p>Prior studies introduces 2-level quantization in the group-wise scaling factors to reduce the memory footprint. As a result, \\(Q_{w_{s4}}\\) is directly dequantized to <code>FP16</code>:</p> <p>$$ \\hat{\\mathbf{W}}=\\mathbf{Q}{\\mathbf{W}}4}}\\cdot\\mathbf{s{\\mathrm{fp}16},\\quad\\mathbf{\\hat{s}}}16}=\\mathbf{s{\\mathrm{u}8}^{(1)}\\cdot\\mathbf{s} $$ Drawbacks:  1. DGQ: seperated kernels 2. QLoRA: failed to use Tensor Core}16}^{(0)</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#smoothattention","title":"SmoothAttention","text":"<p> Key Observation: the Value matrices show no significant outlier pattern,  whereas Key matrices tend to have fixed outlier channels in each head.</p> \\[ \\mathbf{Z}=\\left(\\mathbf{Q}\\boldsymbol{\\Lambda}\\right)\\cdot\\left(\\mathbf{K}\\boldsymbol{\\Lambda}^{-1}\\right)^{T},\\quad\\boldsymbol{\\Lambda}=\\mathrm{diag}\\left(\\lambda\\right) ,\\quad \\lambda_i=\\max\\left(|\\mathbf{K}_i|\\right)^\\alpha. \\] <p>To be compatible with ROPE,  $$ \\lambda_i=\\lambda_{i+\\frac D2}=\\max\\left(\\max\\left(|\\mathbf{K}i|\\right),\\max\\left(|\\mathbf{K}|\\right)\\right)^\\alpha  $$</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#general-optimizations","title":"General optimizations","text":""},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#1-input-module-rotation","title":"1. Input Module Rotation","text":"<p>Quote</p> <p>We rotate the corresponding weights accordingly in the reversed direction. After rotation, each channel\u2019s activations are linear combinations of all other channels, and thus outlier channels are effectively suppressed. Furthermore, since rotation is a unitary transformation, we can fuse the rotation matrix with the previous linear layers\u2019 weights. We simply choose the scaled Hadamard matrix as the rotation matrix.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#2-ouput-module-smoothing","title":"2. Ouput Module Smoothing","text":"<p>smooth matrix \\(\\Lambda\\) is calculated in SmoothQuant:</p> \\[ \\mathbf{\\lambda }_j=\\max(|\\mathbf{X}_j|)^\\alpha/\\max(|\\mathbf{W}_j|)^{1-\\alpha} \\] <p>In QServe's implementation, \\(\\alpha\\) which is nearly 0 is better.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#3-activation-channel-reordering","title":"3. Activation Channel Reordering","text":"<p>Quote</p> <p>We use max (\\(|\\mathbf{X}|\\)) to determine the channel salience, and then reorder channels so that channels with similar salience are in the same quantization group.</p>"},{"location":"2025/12/16/qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving/#qserve-serving-system","title":"QServe Serving System","text":""},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/simulation/","title":"Simulation","text":""},{"location":"category/gpu/","title":"GPU","text":""},{"location":"category/cuda/","title":"CUDA","text":""},{"location":"category/llm-inference/","title":"LLM Inference","text":""},{"location":"category/phd-application/","title":"PhD Application","text":""},{"location":"category/note/","title":"NOTE","text":""},{"location":"category/toefl/","title":"TOEFL","text":""},{"location":"category/mlsys/","title":"mlsys","text":""}]}